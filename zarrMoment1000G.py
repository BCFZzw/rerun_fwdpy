import h5py
import pandas as pd
import numpy as np
import allel
import moments.LD
import os
from scipy import interpolate
import zarr

def parseNYGCRecMap(recFile):
    """
    From NYGC format recombination map, return pandas dataframe of cM and bp
    strict Position not currently in use
    """
    recMap = pd.read_csv(recFile, sep=' ', header = None, usecols = [2, 3]) 
    recMap.columns = ["cM", "bp"]
    #f = interpolate.interp1d(recMap.bp, recMap.cM, fill_value='extrapolate')
    return recMap #, f


#for non jobID
def getBinEdgeBp(recMap, posStart, posEnd, binSize):
    bpTocMF = interpolate.interp1d(recMap.bp, recMap.cM, fill_value='extrapolate')
    cM = recMap.cM
    bp = recMap.bp
    cMTobpF = interpolate.interp1d(recMap.cM, recMap.bp, fill_value='extrapolate')

    #when to do the steps to filter out variants
    start = bpTocMF(pos[0])
    end = bpTocMF(pos[-1])
    binNum = int((end- start)/ binSize) + 1 
    #calculated as cM bincounts, thus, transform back to bp and take the leftmost value

    assert binNum/jobSize >= 1
    #with + 1 above, do not need to add 1 again
    binEdgeCM = np.array([start + i * binSize for i in range(0, binNum)])
    binEdgeBp = cMTobpF(binEdgeCM).astype(int)

    return binEdgeBp


def jobPosRange(recMap, pos, jobSize, jobID, binSize = 0.01):
    """
    Require zarr formated callset, jobSize, and jobID for parallelization
    """
    assert jobID >= 1
    assert jobSize >=1

    bpTocMF = interpolate.interp1d(recMap.bp, recMap.cM, fill_value='extrapolate')
    cM = recMap.cM
    bp = recMap.bp
    cMTobpF = interpolate.interp1d(recMap.cM, recMap.bp, fill_value='extrapolate')

    #when to do the steps to filter out variants
    start = bpTocMF(pos[0])
    end = bpTocMF(pos[-1])
    binNum = int((end- start)/ binSize) + 1 
    #calculated as cM bincounts, thus, transform back to bp and take the leftmost value

    assert binNum/jobSize >= 1
    

    #get parallelized position range
    step = int(binNum // jobSize)# get each step a job is taking
    #leave the extra one for the last job
    #jobID starts with 1
    # made sure each job starts and end within complete bins
    if jobID != jobSize:
        binStart = step*(jobID-1)
        binEdgeCM = np.array([start + i * binSize for i in range(binStart, binStart + step + 1)]) #this plus one accounts for the finishing bin end
        binEdgeBp = np.around(cMTobpF(binEdgeCM)).astype(int) #casted to int, the case where duplicate value will be tested
    else:
        binStart = step*(jobID -1)*binSize + start
        binEdgeCM = np.arange(binStart, end, binSize) #this will for sure exclude the end
        binEdgeBp = np.around(cMTobpF(binEdgeCM)).astype(int) #sometimes rounding will make the last element == end
        #make the last bin of the last job slightly bigger than the rest
        if pos[-1] - binEdgeBp[-1] >= cMTobpF(binSize):
            binEdgeCM = np.append(binEdgeCM, end)
            binEdgeBp = np.append(binEdgeBp, pos[-1])
        else:
            binEdgeBp[-1] = pos[-1]
            binEdgeCM[-1] = end
    
    return binEdgeCM, binEdgeBp


def pandasParsing(subsetPos, binEdgeBp):
    """
    Achieve segmentation via pandas's groupby and cut functions
    Bin range, position data is saved to csv meta file
    Also utilizes a few function from scikit allel for subsetting
    """

    df = pd.DataFrame(subsetPos, columns =  ["positions"])
    #@TODO: kept non polymorphic but skip with moments
    labels = pd.cut(subsetPos, binEdgeBp, right = False)
    df["labels"] = labels
    groupDf = df.groupby("labels")
    df["counts"] = groupDf.transform("count")

    return df

#this df is generated by pandasParsing(subsetPos, binEdgeBp), and used its index to have the subsetGeno
def parseAndMoment(df, subsetGeno, filePrefix = "", savePath = "", save = True):
    """
    Achieve segmentation via pandas's groupby and cut functions
    Bin range, position data is saved to csv meta file
    @TODO: improve performance by unsorting groupby
    """

    size = len(df)
    D2 = np.empty(shape = size)
    Dz = np.empty(shape = size)
    Pi2 = np.empty(shape = size)
    D = np.empty(shape = size)
    medianPos = np.empty(shape = size)
    snpCounts = np.empty(shape = size)
    
    indicesList = df.groupby("labels").indices.values() 
    counter = 0

    for indices in indicesList: #@TODO possibly not optimal
        if len(indices) < 2:
            continue
        
        data = np.take(subsetGeno, indices, axis = 0) #optimize?
        assert np.shape(data)[0] == len(indices)
        stats = moments.LD.Parsing.compute_pairwise_stats(data, genotypes = True)
        #I remember this is faster than df[idx] = ... each time 
        D2[counter] = np.sum(stats[0]) #sum up every possible pair values
        Dz[counter] = np.sum(stats[1])
        Pi2[counter] = np.sum(stats[2])
        D[counter] = np.sum(stats[3])
        medianPos[counter] = np.median(df["positions"][indices]).astype(int)
        snpCounts[counter] = df["counts"][indices[0]]
        counter = counter + 1
    
    D2 = D2[:counter]
    Dz = Dz[:counter]
    Pi2 = Pi2[:counter]
    D = D[:counter]
    medianPos = medianPos[:counter]
    snpCounts = snpCounts[:counter]

    statsDf = pd.DataFrame({"medPos": medianPos,
                            "snpCounts": snpCounts,
                            "D2":D2,
                            "Dz": Dz,
                            "Pi2": Pi2,
                            "D": D
                            })

    if save:
        hf = h5py.File(os.path.join(savePath, filePrefix + "_LD.h5"), "w")
        hf.create_dataset('D2', data = D2)
        hf.create_dataset('Dz', data = Dz)
        hf.create_dataset('Pi2', data = Pi2)
        hf.create_dataset('D', data = D)
        hf.create_dataset('medPosition', data = medianPos)
        hf.create_dataset('snpCounts', data = snpCounts)
        hf.close()

    return statsDf

#tested
def locateSubsetBoolean(pos, binEdgeBp):
    #this utilizes the allel funciton of locating multiple ranges check if range does not exist 
    #this return an boolean array
    #this can pass an empty array
    return pos.locate_ranges([binEdgeBp[0]], [binEdgeBp[-1]], strict = False)
    
#@TODO: not tested
def popSubsetIndex(samples, panel, popList):
    samples_list = list(samples)
    samples_callset_index = [samples_list.index(s) for s in panel['SampleID']]
    panel['callset_index'] = samples_callset_index

    return  panel[panel.SampleID.isin(popList)].callset_index.values

#tested
#@TODO double check, excluding polymorphic sites can further optimize procedures
def subsetPosGeno(popIndex, daskGenotype, position, binEdgeBp):
    """
    The position and genotype are subsetted to only the segments used in the job
    Hence, their variants size are equal, and only the indices are used to parse in the next function
    """
    posBool = locateSubsetBoolean(position, binEdgeBp)
    assert np.any(posBool) # at least some elements exist in the selected regions
    assert np.shape(posBool)[0] == np.shape(position)[0]
    subsetPos = position[posBool] #subset genotype also using posbool
    #selecting on boolean, use compress axis = 0; selecting on index, use take, axis = 1 ?
    subsetGeno = daskGenotype.subset(posBool, popIndex).compute()
    assert np.shape(subsetGeno)[0] == np.shape(subsetPos)[0]
    assert np.shape(subsetGeno)[1] == np.shape(popIndex)[0]
    assert np.shape(subsetGeno)[2] == 2 # ploidity 

    return subsetPos, subsetGeno

def normStatsDf(statsDf, recMap):
    bpTocMF = interpolate.interp1d(recMap.bp, recMap.cM, fill_value='extrapolate')
    statsDf["cMMedPos"] = bpTocMF(statsDf.medPos)
    statsDf["sigD2"] = statsDf["D2"]/statsDf["Pi2"]
    statsDf["sigDz"] = statsDf["Dz"]/statsDf["Pi2"] 
    return statsDf

def zarrPipeline(callset, panel, popList, recFile, binSize, jobSize, jobID, filePrefix, savePath, save = True):
    samples = callset['samples'][:]
    popIndex = popSubsetIndex(samples, panel, popList)
    #in this order, the file can be read and processed fairly quickly
    genotype = allel.GenotypeChunkedArray(callset['calldata/GT'])
    daskGenotype = allel.GenotypeDaskArray(genotype)
    position = allel.SortedIndex(callset['variants/POS'])
    assert np.shape(daskGenotype)[0] == np.shape(position)[0]
    recMap = parseNYGCRecMap(recFile)
    binEdgeCM, binEdgeBp = jobPosRange(recMap, position, jobSize, jobID, binSize)
    subsetPos, subsetGeno = subsetPosGeno(popIndex, daskGenotype, position, binEdgeBp)
    print("subsetting complete")

    df = pandasParsing(subsetPos, binEdgeBp)
    print("Bins created: "+ str(df.labels.nunique()))
    print("Loading complete")

    subsetGeno012 = subsetGeno.to_n_alt(fill=-1) #slowing step as well, takes about 4 mins to complete 40k*3202*2 entries

    statsDf = parseAndMoment(df, subsetGeno012, filePrefix, savePath, save)

    finalStatsDf = normStatsDf(statsDf, recMap)

    return finalStatsDf